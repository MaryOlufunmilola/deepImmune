{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from ... import data_generator as gn\n",
    "# import data_generator_pytorch as gn\n",
    "# import datetime\n",
    "# import time\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "from evaluate_attention import evaluate_attention\n",
    "from tensorboardX import SummaryWriter\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "# import importlib\n",
    "import model.net as net\n",
    "import utils\n",
    "# import model.data_loader as data_loader\n",
    "import model.data_generator as data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To process argparse \n",
    "https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--restore_file'], dest='restore_file', nargs=None, const=None, default=None, type=None, choices=None, help='Optional,                     full path  of file  oR                      name of the file in --model_dir (withouth ext .pth.tar)                     containing weights to reload before                     training', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', default='data/64x64_SIGNS',\n",
    "                    help=\"File containing directory containing datasets\")\n",
    "# parser.add_argument('--data_dir_list', default=None,\n",
    "# help=\"File contating list of dataset directories data_dirs\")\n",
    "parser.add_argument('--model_dir', default='experiments/base_model',\n",
    "                    help=\"Directory containing params.json\")\n",
    "parser.add_argument('--tensorboard_prefix', default='',\n",
    "                    help=\"prefix for tensorboard logging\")\n",
    "parser.add_argument('--hyper_param', default='',\n",
    "                    help=\"support string for setting parameter from command line e.g.\\\"params.input_indices=range(50)\\\"\")\n",
    "parser.add_argument('--prefix', default='',\n",
    "                    help=\"Prefix of dataset files  \\n \\\n",
    "                    (e.g. prefix=\\\"tcga\\\" implies input files are \\n \\\n",
    "                    tcga_ssgsea_[train,test,val].txt, \\n \\\n",
    "                    tcga_phenotype_[train,test,val].txt )\")\n",
    "parser.add_argument('--restore_file', default=None,\n",
    "                    help=\"Optional, \\\n",
    "                    full path  of file  oR  \\\n",
    "                    name of the file in --model_dir (withouth ext .pth.tar) \\\n",
    "                    containing weights to reload before \\\n",
    "                    training\")  # 'best' or 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train_attention.py  --data_dir  ../data/tcga/neoantigen.v2/attention/tcga.imputed/datasets_tsne_list.txt --model_dir ../data/tcga/neoantigen.v2/attention/tcga.imputed/.\n",
    "command =\"--data_dir  \\\n",
    "../data/tcga/neoantigen.v2/attention/tcga.imputed/datasets_tsne_list.txt \\\n",
    "--model_dir ../data/tcga/neoantigen.v2/attention/tcga.imputed/.  \\\n",
    "--restore_file ../data/tcga/neoantigen.v2/attention/tcga.imputed/tensorboardLog/SGD_20190426-131226//best.pth.tar\"\n",
    "args = parser.parse_args(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention(models, optimizers, dataloader, metrics, params, train_optimizer_mask):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    for inx in range(len(models)):\n",
    "        models[inx].train() if train_optimizer_mask[inx] else models[inx].eval()\n",
    "\n",
    "    num_batches_per_epoch, _, _, dataloader = dataloader\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    # net.tracer()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "\n",
    "    with tqdm(total=num_batches_per_epoch) as t:\n",
    "        for i, (features, all_labels, _) in zip(range(num_batches_per_epoch), dataloader):\n",
    "            # survival = np.take(all_labels, params.survival_indices, axis=1) if len(params.survival_indices) else None\n",
    "            # labels_san_survival = np.take(all_labels, params.survival_indices + params.continuous_phenotype_indices + params.binary_phenotype_indices, axis=1).astype(float)\n",
    "            labels_san_survival = all_labels\n",
    "            # net.tracer()\n",
    "            train_batch, labels_batch = torch.from_numpy(\n",
    "                features).float(), torch.from_numpy(labels_san_survival).float()\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(\n",
    "                    non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
    "            # convert to torch Variables\n",
    "\n",
    "            embedding_input = train_batch[:, params.embedding_indices]\n",
    "            attention_input = train_batch[:, params.attention_indices]\n",
    "\n",
    "            embedding_batch = models[0](embedding_input)\n",
    "            attention_mat = models[1](attention_input)\n",
    "            transformed_batch = net.feature_attention(attention_mat, embedding_batch)\n",
    "            output_batch = models[2](transformed_batch)\n",
    "\n",
    "            loss = net.update_loss_parameters_vectorized(labels_batch, output_batch, models, optimizers, params, train_optimizer_mask)\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # net.  tracer()\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {dd[0] + \"_\" + dd[1]: metrics[dd[1]](output_batch[:, dd[2]], labels_san_survival[:, dd[3]: (dd[3] + 2)])\n",
    "                                 if dd[1] == 'c_index' else\n",
    "                                 metrics[dd[1]](output_batch[:, dd[2]], labels_san_survival[:, dd[3]])\n",
    "                                 for inx, dd in enumerate(params.metrics)}\n",
    "\n",
    "                summary_batch['loss'] = loss\n",
    "                summary_batch['negative_loss'] = -loss\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss)\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: net.mean_na([x[metric]\n",
    "                                         for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "    return metrics_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(models, optimizers, datasets, metrics, params, model_dir, tensorboard_dir,\n",
    "                       restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        datasets : list of dataloaders, each containing train_dataloader and val_dataloader\n",
    "        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    print(restore_file)\n",
    "    if restore_file is not None:\n",
    "        if os.path.isfile(restore_file):\n",
    "            restore_path = restore_file\n",
    "        else:\n",
    "            restore_path = os.path.join(\n",
    "                model_dir, restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint_attn(restore_path, models)  # not updating the optimizers for flexiblity\n",
    "        # utils.load_checkpoint(restore_path, embedding_model, outputs, optimizer)\n",
    "\n",
    "    best_val_acc = None  # for cindex\n",
    "    # tsne_params_log = 5\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        train_metrics_all = []\n",
    "        val_metrics_all = []\n",
    "        for index, dataset in enumerate(datasets):\n",
    "            # compute number of batches in one epoch (one full pass over the training set)\n",
    "            dataloader, train_optimizer_mask, (dataset_name, tsne) = dataset\n",
    "            if 'train' in dataloader.keys():\n",
    "                train_metrics = train_attention(models, optimizers, dataloader['train'], metrics, params, train_optimizer_mask)\n",
    "                train_metrics_all.append(train_metrics)\n",
    "                if params.tensorboardlog[0]:\n",
    "                    writer.add_scalars('train_' + str(index), train_metrics, epoch)\n",
    "\n",
    "            # Evaluate for one epoch on validation set\n",
    "            if 'val' in dataloader.keys():\n",
    "                validation_file = os.path.join(tensorboard_dir, \"last_val_{0}.csv\".format(index)) if (epoch >= params.num_epochs - 1) else None\n",
    "                val_metrics = evaluate_attention(models, dataloader['val'], metrics, params, validation_file, writer=writer, epoch=epoch, index=index, tsne=tsne)\n",
    "                val_metrics_all.append(val_metrics)\n",
    "                if params.tensorboardlog[0]:\n",
    "                    writer.add_scalars('val_' + str(index), val_metrics, epoch)\n",
    "\n",
    "        # net.tracer()\n",
    "\n",
    "        model_names = [\"encoder\", \"attention\", \"decoder\"]\n",
    "\n",
    "        if params.tensorboardlog[1]:\n",
    "            if epoch % params.save_summary_steps == 0:\n",
    "\n",
    "                for model_name, model in zip(model_names, models):\n",
    "\n",
    "                    for name, param1 in model.named_parameters():\n",
    "                        try:\n",
    "                            writer.add_histogram(model_name + \"/\" + name, param1.clone().cpu().data.numpy(), epoch)\n",
    "                            writer.add_histogram(\"grad/\" + model_name + \"/\" + name, param1.grad.clone().cpu().data.numpy(), epoch)\n",
    "                        except:\n",
    "                            # print(\"error in writing histogram\")\n",
    "                            pass\n",
    "\n",
    "        val_metrics = {metric: eval(params.aggregate)([x[metric] for x in val_metrics_all]) for metric in val_metrics_all[0]}\n",
    "\n",
    "        # val_metrics = eval(params.aggregate)(val_metrics)\n",
    "        # val_acc = val_metrics[params.best_model_metric]  # use differnt functions\n",
    "        # val_acc = min(val_metrics['c_index'], val_metrics['auc'])  # use differnt functions\n",
    "        val_acc = val_metrics[params.best_model_metric]\n",
    "        if best_val_acc is None:\n",
    "            is_best = True\n",
    "        else:\n",
    "            is_best = val_acc > best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'embedding_state_dict': models[0].state_dict(),\n",
    "                               'attention_state_dict': models[1].state_dict(),\n",
    "                               'outputs_state_dict': models[2].state_dict(),\n",
    "                               'embedding_optim_dict': optimizers[0].state_dict(),\n",
    "                               'attention_optim_dict': optimizers[1].state_dict(),\n",
    "                               'outputs_optim_dict': optimizers[2].state_dict()\n",
    "                               },\n",
    "                              is_best=is_best,\n",
    "                              checkpoint=tensorboard_dir)\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best metric {} {}\".format(params.best_model_metric, val_acc))\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(\n",
    "                tensorboard_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "            # save best model\n",
    "            best_val_meterics_all = [evaluate_attention(models, dataset[0]['val'], metrics, params, os.path.join(tensorboard_dir, \"best_val_{0}.csv\".format(index))) for index, dataset in enumerate(datasets)]\n",
    "            best_json_path_dataset = os.path.join(\n",
    "                tensorboard_dir, \"metrics_val_best_weights_datasets.json\")\n",
    "            utils.save_dict_to_json(best_val_meterics_all, best_json_path_dataset)\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(\n",
    "            tensorboard_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)\n",
    "\n",
    "        # return the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import cuda module: No module named 'caffe2.python.caffe2_pybind11_state_gpu'\n",
      "Failed to import AMD hip module: No module named 'caffe2.python.caffe2_pybind11_state_hip'\n",
      "This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "Tensorboard logging directory ../data/tcga/neoantigen.v2/attention/tcga.imputed/./tensorboardLog/20190506-083816\n",
      "Loading the datasets...\n",
      "Found 1 datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/tcga/neoantigen.v2/attention/genentech.imputed/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/tcga/neoantigen.v2/attention/genentech.imputed/\n",
      "\n",
      "\n",
      "initial output size\n",
      "131\n",
      "final convolution layer output size\n",
      "initial fully connected size\n",
      "131\n",
      "16\n",
      "16\n",
      "Embedding size\n",
      "initial output size\n",
      "33\n",
      "final convolution layer output size\n",
      "initial fully connected size\n",
      "33\n",
      "16\n",
      "16\n",
      "Embedding size\n",
      "initial output size\n",
      "8\n",
      "final convolution layer output size\n",
      "initial fully connected size\n",
      "8\n",
      "Embedding size\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "json_path = os.path.join(args.model_dir, 'params.json')\n",
    "assert os.path.isfile(\n",
    "    json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "params.cuda = torch.cuda.is_available()\n",
    "exec(args.hyper_param)\n",
    "params = net.create_lossfns_mask(params)\n",
    "\n",
    "\n",
    "    # Set the logger\n",
    "utils.set_logger(os.path.join(args.model_dir, 'train.log'))\n",
    "tensorboard_dir = os.path.join(args.model_dir, 'tensorboardLog', args.tensorboard_prefix + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(tensorboard_dir)\n",
    "copy(json_path, tensorboard_dir)\n",
    "copy(args.data_dir, tensorboard_dir)\n",
    "logging.info(\"Tensorboard logging directory {}\".format(tensorboard_dir))\n",
    "\n",
    "utils.save_dict_to_json(args.hyper_param, os.path.join(\n",
    "    tensorboard_dir, \"hyper_param.txt\"))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# fetch dataloaders\n",
    "datasets = data_generator.fetch_dataloader_list(args.prefix,\n",
    "                                                ['train', 'val'], args.data_dir, params)\n",
    "_, _, params.header, _ = datasets[0][0]['train']\n",
    "params.input_size = len(params.embedding_indices)\n",
    "# net.tracer()\n",
    "params.attention_input_size = len(params.attention_indices)\n",
    "params = net.define_metrics(params)\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "# Define the model and optimizer\n",
    "modelClasses = [net.EmbeddingNet, net.AttentionEncoder, net.outputLayer]\n",
    "models = [modelClass(params).cuda() if params.cuda else modelClass(params) for modelClass in modelClasses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal_layers.FC_block1.0.fc1.weight\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.0.fc1.bias\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.0.norm2.weight\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.0.norm2.bias\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.1.fc1.weight\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.1.fc1.bias\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.1.norm2.weight\n",
      ".............. here ...............\n",
      "internal_layers.FC_block1.1.norm2.bias\n",
      ".............. here ...............\n",
      "internal_layers.norm.weight\n",
      ".............. here ...............\n",
      "internal_layers.norm.bias\n",
      ".............. here ...............\n",
      "internal_layers.fc3.weight\n",
      ".............. here ...............\n",
      "internal_layers.fc3.bias\n",
      ".............. here ...............\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# print(models[1].modules())\n",
    "i = 0\n",
    "for   xx, ii in models[1].named_parameters() : \n",
    "    i = i + 1\n",
    "    print(xx)\n",
    "#     print(ii)\n",
    "    print(\".............. here ...............\")\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-76-249754bebcf4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-76-249754bebcf4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(models[1].internal_layers.FC_block1.0)\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(models[1].internal_layers.FC_block1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "2. Unit test cases \n",
    "3. Manually run the backprop, sending survival and response from genentech. Once in a while send back the tcga. Monitor the changes in gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # vectorized  optimizers\n",
    "    optimizers = [optim.SGD(\n",
    "        model.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay, momentum=0.9) for model in models]\n",
    "\n",
    "    # fetch loss function and metrics\n",
    "    metrics = net.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layerwise learning\n",
    "1. lr(n-1) = lr(n)/2.6\n",
    "2. triangular learning \n",
    "3. Gradual unfreezing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizers(models, params, optim_config=None) :\n",
    "    if optim_config is None : \n",
    "        optimizers = [optim.SGD(\n",
    "        model.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay, momentum=0.9) for model in models]\n",
    "    else :\n",
    "        optimizers = [optim.SGD(\n",
    "        model.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay, momentum=0.9) for model in models]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tcga/neoantigen.v2/attention/tcga.imputed/tensorboardLog/SGD_20190426-131226//best.pth.tar\n",
      "../data/tcga/neoantigen.v2/attention/tcga.imputed/.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'restore_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1952e0477258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'restore_file' is not defined"
     ]
    }
   ],
   "source": [
    "args.restore_file = \"../data/tcga/neoantigen.v2/attention/tcga.imputed/tensorboardLog/SGD_20190426-131226//best.pth.tar\"\n",
    "print(args.restore_file)\n",
    "print(args.model_dir)\n",
    "os.path.isfile(restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ../data/tcga/neoantigen.v2/attention/tcga.imputed/tensorboardLog/SGD_20190426-131226//best.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tcga/neoantigen.v2/attention/tcga.imputed/tensorboardLog/SGD_20190426-131226//best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# train_and_evaluate(models, optimizers, datasets, metrics, params, args.model_dir, tensorboard_dir,\n",
    "#                        args.restore_file)\n",
    "restore_file = args.restore_file\n",
    "model_dir = args.model_dir\n",
    "print(restore_file)\n",
    "if restore_file is not None:\n",
    "    if os.path.isfile(restore_file):\n",
    "        restore_path = restore_file\n",
    "    else:\n",
    "        restore_path = os.path.join(\n",
    "            model_dir, restore_file + '.pth.tar')\n",
    "    logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "    utils.load_checkpoint_attn(restore_path, models, params)  # not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "model_names = [\"encoder\", \"attention\", \"decoder\"]\n",
    "\n",
    "for model_name, model in zip(model_names, models):\n",
    "\n",
    "    for name, param1 in model.named_parameters():\n",
    "        try:\n",
    "            writer.add_histogram(model_name + \"/\" + name, param1.clone().cpu().data.numpy(), epoch)\n",
    "#             writer.add_histogram(\"grad/\" + model_name + \"/\" + name, param1.grad.clone().cpu().data.numpy(), epoch)\n",
    "        except:\n",
    "            print(\"error in writing histogram\")\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE based data balancing\n",
    "1. Only in training set\n",
    "2. Also in test set \n",
    "3. Add guassian noise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "# data, labels = sm.fit_sample(data, labels)\n",
    "def is_categorical(xx):\n",
    "    return any([isinstance(uu, str) for uu in xx])\n",
    "#     return len(set(xx)) > 0 and xx.dtype != np.number \n",
    "\n",
    "\n",
    "def cat2int(xx):\n",
    "    xx = pd.Series(xx).astype(\"category\")\n",
    "    xx_int = xx.cat.codes\n",
    "    maped = dict( enumerate(xx.cat.categories ) )\n",
    "    \n",
    "    return xx_int, maped\n",
    "\n",
    "def int2cat(xx, maped) :\n",
    "    xx = pd.Series(xx).astype(\"category\")\n",
    "    xx = xx.cat.codes.map(maped)\n",
    "    return xx\n",
    "    \n",
    "\n",
    "def DataAugmentation(data, labels, balance=False):\n",
    "#     ipdb.set_trace()\n",
    "    categorical_features = [ is_categorical(data[:,inx]) for inx in range(data.shape[1])]\n",
    "    categorical_features_index = np.where(categorical_features)[0]\n",
    "    labels = labels.astype('float32')\n",
    "    na_inx = np.isnan(labels)\n",
    "    data_na, labels_na = data[na_inx], labels[na_inx]\n",
    "    data1, labels1= data[np.logical_not(na_inx)], labels[np.logical_not(na_inx)]\n",
    "\n",
    "    \n",
    "    if len(labels1 > 2) : \n",
    "        if balance:\n",
    "            data1 = np.nan_to_num(data1, copy=False)\n",
    "            data1 = pd.DataFrame(data1)\n",
    "            data1 = data1.fillna(0)\n",
    "            mappeds = []\n",
    "            for ii in categorical_features_index : \n",
    "                data1[ii], mapped = cat2int(data1[ii])\n",
    "                mappeds.append(mapped)\n",
    "            # imputation\n",
    "            sm = SMOTENC(random_state=42, categorical_features=categorical_features)\n",
    "    #         sm = SMOTETomek(ratio='auto')\n",
    "            data1, labels1 = sm.fit_sample(data1, labels1)\n",
    "            data1 = pd.DataFrame(data1) \n",
    "            for mapped, ii in zip(mappeds, categorical_features_index) : \n",
    "                data1[ii] = int2cat(data1[ii],mapped)\n",
    "            data1  = data1.values\n",
    "\n",
    "        data = np.concatenate([data1, data_na], 0)\n",
    "        labels = np.concatenate([labels1, labels_na], 0)\n",
    "    \n",
    "    return data, labels\n",
    "    \n",
    "\n",
    "def ncols(xx):\n",
    "    return xx.shape[1]\n",
    "\n",
    "def nrows(xx):\n",
    "    return xx.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import ipdb\n",
    "def generator_survival_new(features, labels, params, cancertype=None, \n",
    "                       shuffle=True, batch_size=64, batch_by_type=False, \n",
    "                       normalize_input=False, dataset_type='non_icb', sort_survival=False, \n",
    "                       header=None, tsne_labels_mat=None, balance=False, data_augmentation = False):\n",
    "    \"\"\"\n",
    "    Parses the input file and creates a generator for the input file\n",
    "\n",
    "    Returns:\n",
    "    num_batches_per_epoch -- The number of batches per epoch based on data size\n",
    "    input_size -- the dimensions of the input data\n",
    "    it also sort the survival data\n",
    "    data_generator() -- the generator function to yield the features and labels\n",
    "    \"\"\"\n",
    "    # np.random.seed(230)\n",
    "    # tracer()\n",
    "\n",
    "    def create_batches(feat, lab, tsne_mat, batch_size, shuffle=True):\n",
    "    \n",
    "        lab_survival, lab_continuous, lab_binary = \\\n",
    "            np_take(lab, params.survival_indices, axis=1), \\\n",
    "            np_take(lab, params.continuous_phenotype_indices, axis=1),\\\n",
    "            np_take(lab, params.binary_phenotype_indices, axis=1)\n",
    "        lab_continuous = quantile_normalize(lab_continuous)\n",
    "        # lab_continuous = quantile_normalize(lab_continuous, method=\"znorm\")\n",
    "        \n",
    "        lab = np.concatenate([lab_survival, lab_continuous, lab_binary], 1).astype(float)\n",
    "        # tracer()\n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        if data_augmentation and len(params.binary_phenotype_indices) > 0 : \n",
    "\n",
    "            labels_aug = lab_binary[:,0]            \n",
    "            data = np.concatenate([feat, tsne_mat, lab_survival, lab_continuous, lab_binary[:,1:]], 1)\n",
    "            data, labels_aug = DataAugmentation(data, labels_aug, balance=balance)\n",
    "#             ipdb.set_trace()\n",
    "            feat= data[:,:ncols(feat)]\n",
    "            tsne_mat = data[:,ncols(feat): (ncols(feat)+ ncols(tsne_mat))]\n",
    "            lab_augumented = data[:,ncols(feat)+ ncols(tsne_mat):]\n",
    "            lab = np.concatenate([lab_augumented[:, : ncols(lab_survival) + ncols(lab_continuous)], \n",
    "            labels_aug.reshape(-1,1), lab_augumented[:, ncols(lab_survival) + ncols(lab_continuous):]],1)\n",
    "            \n",
    "        data_size = len(feat)\n",
    "        num_batches_per_epoch = max(1, int((data_size - 1) / batch_size))\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            feat, lab, tsne_mat = feat[shuffle_indices], lab[shuffle_indices], tsne_mat[shuffle_indices]\n",
    " \n",
    "        batches_curr = []\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = (batch_num + 1) * batch_size\n",
    "            if batch_num == num_batches_per_epoch - 1:\n",
    "                end_index = data_size\n",
    "\n",
    "            temp = (feat[start_index:end_index], lab[start_index:end_index], tsne_mat[start_index:end_index])\n",
    "            batches_curr.append(temp)\n",
    "\n",
    "        return batches_curr\n",
    "\n",
    "    def get_batches(features, labels):\n",
    "\n",
    "        features = features.astype(float)\n",
    "        features = np.nan_to_num(features)  # convert NANs to zeros\n",
    "        # tracer()\n",
    "        survival_time_index = np.take(params.survival_indices, range(0, len(params.survival_indices), 2))\n",
    "        if normalize_input:\n",
    "            # normalization by type\n",
    "            # features = quantile_normalize(features)\n",
    "            for type_curr in types:\n",
    "                # print(type_curr)\n",
    "                features[cancertype == type_curr] = quantile_normalize_nonbinary(features[cancertype == type_curr])\n",
    "                for ii in range(len(survival_time_index)):\n",
    "                    labels[cancertype == type_curr, ii] = quantile_normalize(labels[cancertype == type_curr, ii])\n",
    "\n",
    "        if batch_by_type:\n",
    "            batches = [Xy for type_curr in types for Xy in create_batches(features[cancertype == type_curr], labels[cancertype == type_curr], tsne_labels_mat[cancertype == type_curr], batch_size, shuffle)]\n",
    "        else:\n",
    "            batches = create_batches(features, labels, tsne_labels_mat, batch_size, shuffle)\n",
    "        return batches\n",
    "\n",
    "    if params.input_indices != \"None\":  # use == because param.input_indices is unicode\n",
    "        features = np_take(features, params.input_indices, axis=1)\n",
    "\n",
    "    if cancertype is None:\n",
    "        raise NameError(\"cancertype not found\")\n",
    "    # types = cancertype.dtype.categories\n",
    "    else:\n",
    "        types = set(cancertype)\n",
    "    batches = get_batches(features, labels)\n",
    "    data_size = len(features)\n",
    "    num_batches_per_epoch = len(batches)\n",
    "    input_size = features.shape[1]\n",
    "\n",
    "    if header is not None:\n",
    "        header1 = np.array(header[1:])\n",
    "        header = np.concatenate(\n",
    "            [np_take(header1, params.survival_indices),\n",
    "             np_take(header1, params.continuous_phenotype_indices),\n",
    "             np_take(header1, params.binary_phenotype_indices)])\n",
    "\n",
    "    # Sorts the batches by survival time\n",
    "    def data_generator():\n",
    "        while True:\n",
    "            batches = get_batches(features, labels)\n",
    "            for X, y, tsne_mat in batches:\n",
    "                # X, y = batch[0]\n",
    "                # tracer()\n",
    "                if sort_survival:\n",
    "                    sort_index = (input_size - 3) if dataset_type is 'icb' else 0\n",
    "                    # this was assuming in icb dataset survival is done through\n",
    "                    idx = np.argsort(abs(y[:, sort_index]))[::-1]\n",
    "                    X = X[idx, :]\n",
    "                    # sort by survival time and take censored data\n",
    "                    # y = y[idx, 1].reshape(-1, 1)\n",
    "                    y = y[idx, :]\n",
    "                    tsne_mat = tsne_mat[idx, :]\n",
    "\n",
    "                yield X, y, tsne_mat\n",
    "\n",
    "    return num_batches_per_epoch, input_size, header, data_generator()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "npzfile = np.load(\"temp.npz\")\n",
    "print(npzfile.files)\n",
    "data, labels, balance = npzfile['arr_0'], npzfile['arr_1'], npzfile['arr_2']\n",
    "print(balance)\n",
    "# print(data1)\n",
    "print(type(data1.values))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "categorical_features = [ is_categorical(data[:,inx]) for inx in range(data.shape[1])]\n",
    "categorical_features_index = np.where(categorical_features)[0]\n",
    "labels = labels.astype('float32')\n",
    "na_inx = np.isnan(labels)\n",
    "data_na, labels_na = data[na_inx], labels[na_inx]\n",
    "data1, labels1= data[np.logical_not(na_inx)], labels[np.logical_not(na_inx)]\n",
    "data1 = np.nan_to_num(data1, copy=False)\n",
    "data1 = pd.DataFrame(data1)\n",
    "data1 = data1.fillna(0)\n",
    "mappeds = []\n",
    "for ii in categorical_features_index : \n",
    "    data1[ii], mapped = cat2int(data1[ii])\n",
    "    mappeds.append(mapped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sm = SMOTENC(random_state=42, categorical_features=categorical_features)\n",
    "data1, labels1 = sm.fit_sample(data1, labels1)\n",
    "data1 = pd.DataFrame(data1) \n",
    "for mapped, ii in zip(mappeds, categorical_features_index) : \n",
    "    data1[ii] = int2cat(data1[ii],mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = range(10)\n",
    "print(aa[:5])\n",
    "print(aa[5:6])\n",
    "print(aa[6:])\n",
    "import model\n",
    "from model.data_generator  import *\n",
    "\n",
    "dataset_type = \"icb\"\n",
    "shuffle =True\n",
    "dataset_file = \"../data/tcga/neoantigen.v2/attention/genentech.imputed/dataset_train.txt\"\n",
    "features_phenotypes, header = model.data_generator.readFile(dataset_file, header=True)\n",
    "# phenotypes_type = readFile(path + \"phenotype_\" + split + \".txt\")\n",
    "cancertype = features_phenotypes[:, 0]  # first column in cancertype in the file\n",
    "tsne_labels_mat = np_take(features_phenotypes, params.label_index, axis=1)\n",
    "features_phenotypes = features_phenotypes[:, 1:]\n",
    "dl = generator_survival_new(\n",
    "    features_phenotypes, features_phenotypes, params, batch_by_type=params.batch_by_type, cancertype=cancertype, batch_size=params.batch_size, normalize_input=params.normalize_input, \n",
    "    dataset_type=dataset_type, shuffle=shuffle, header=header, tsne_labels_mat=tsne_labels_mat, balance = True, data_augmentation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'train': (9, 162, array(['survive', 'vital_status', 'nan', 'imputed.response',\n",
      "       'Response_SD/PD', 'Best_CR', 'Best_PR'], dtype='<U39'), <generator object generator_survival.<locals>.data_generator at 0x110e140c0>), 'val': (1, 162, array(['survive', 'vital_status', 'nan', 'imputed.response',\n",
      "       'Response_SD/PD', 'Best_CR', 'Best_PR'], dtype='<U39'), <generator object generator_survival.<locals>.data_generator at 0x110e14b88>)}, [1, 1, 1], (nan, 1))\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(datasets[0])\n",
    "\n",
    "dataset=  datasets[0]\n",
    "dataloader, train_optimizer_mask, (dataset_name, tsne) = dataset\n",
    "params.num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00,  2.09it/s, loss=6.652]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "- Train metrics: survive_c_index: 0.521 ; nan_correlation: 00nan ; imputed.response_auc: 0.591 ; Response_SD/PD_auc: 0.469 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.709 ; negative_loss: -6.709\n",
      "- Eval metrics : survive_c_index: 0.574 ; nan_correlation: 0.000 ; imputed.response_auc: 0.506 ; Response_SD/PD_auc: 0.639 ; Best_CR_auc: 0.348 ; Best_PR_auc: 0.646 ; loss: 6.209 ; negative_loss: -6.209\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00,  1.99it/s, loss=6.551]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "- Train metrics: survive_c_index: 0.524 ; nan_correlation: 00nan ; imputed.response_auc: 0.477 ; Response_SD/PD_auc: 0.569 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.562 ; negative_loss: -6.562\n",
      "- Eval metrics : survive_c_index: 0.574 ; nan_correlation: 0.000 ; imputed.response_auc: 0.549 ; Response_SD/PD_auc: 0.592 ; Best_CR_auc: 0.362 ; Best_PR_auc: 0.596 ; loss: 6.178 ; negative_loss: -6.178\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00, 11.27it/s, loss=6.504]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "- Train metrics: survive_c_index: 0.587 ; nan_correlation: 00nan ; imputed.response_auc: 0.474 ; Response_SD/PD_auc: 0.475 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.496 ; negative_loss: -6.496\n",
      "- Eval metrics : survive_c_index: 0.568 ; nan_correlation: 0.000 ; imputed.response_auc: 0.538 ; Response_SD/PD_auc: 0.573 ; Best_CR_auc: 0.355 ; Best_PR_auc: 0.521 ; loss: 6.188 ; negative_loss: -6.188\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:01<00:00,  5.62it/s, loss=6.354]\n",
      "- Train metrics: survive_c_index: 0.628 ; nan_correlation: 0.000 ; imputed.response_auc: 0.447 ; Response_SD/PD_auc: 0.603 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.337 ; negative_loss: -6.337\n",
      "- Eval metrics : survive_c_index: 0.565 ; nan_correlation: 0.000 ; imputed.response_auc: 0.556 ; Response_SD/PD_auc: 0.523 ; Best_CR_auc: 0.348 ; Best_PR_auc: 0.433 ; loss: 6.193 ; negative_loss: -6.193\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00,  2.38it/s, loss=6.346]\n",
      "- Train metrics: survive_c_index: 0.600 ; nan_correlation: 0.000 ; imputed.response_auc: 0.444 ; Response_SD/PD_auc: 0.557 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.342 ; negative_loss: -6.342\n",
      "- Eval metrics : survive_c_index: 0.551 ; nan_correlation: 0.000 ; imputed.response_auc: 0.564 ; Response_SD/PD_auc: 0.573 ; Best_CR_auc: 0.358 ; Best_PR_auc: 0.442 ; loss: 6.148 ; negative_loss: -6.148\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00, 10.87it/s, loss=6.278]\n",
      "- Train metrics: survive_c_index: 0.592 ; nan_correlation: 0.000 ; imputed.response_auc: 0.462 ; Response_SD/PD_auc: 0.602 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.291 ; negative_loss: -6.291\n",
      "- Eval metrics : survive_c_index: 0.549 ; nan_correlation: 0.000 ; imputed.response_auc: 0.558 ; Response_SD/PD_auc: 0.570 ; Best_CR_auc: 0.365 ; Best_PR_auc: 0.425 ; loss: 6.140 ; negative_loss: -6.140\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00,  2.45it/s, loss=6.245]\n",
      "- Train metrics: survive_c_index: 0.564 ; nan_correlation: 0.000 ; imputed.response_auc: 0.481 ; Response_SD/PD_auc: 0.576 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.287 ; negative_loss: -6.287\n",
      "- Eval metrics : survive_c_index: 0.541 ; nan_correlation: 0.000 ; imputed.response_auc: 0.549 ; Response_SD/PD_auc: 0.595 ; Best_CR_auc: 0.369 ; Best_PR_auc: 0.429 ; loss: 6.125 ; negative_loss: -6.125\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:01<00:00,  8.94it/s, loss=6.197]\n",
      "- Train metrics: survive_c_index: 0.594 ; nan_correlation: 0.000 ; imputed.response_auc: 0.567 ; Response_SD/PD_auc: 0.577 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.209 ; negative_loss: -6.209\n",
      "- Eval metrics : survive_c_index: 0.536 ; nan_correlation: 0.000 ; imputed.response_auc: 0.549 ; Response_SD/PD_auc: 0.581 ; Best_CR_auc: 0.372 ; Best_PR_auc: 0.442 ; loss: 6.115 ; negative_loss: -6.115\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00,  9.01it/s, loss=6.188]\n",
      "- Train metrics: survive_c_index: 0.597 ; nan_correlation: 0.000 ; imputed.response_auc: 0.565 ; Response_SD/PD_auc: 0.581 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.217 ; negative_loss: -6.217\n",
      "- Eval metrics : survive_c_index: 0.540 ; nan_correlation: 0.000 ; imputed.response_auc: 0.571 ; Response_SD/PD_auc: 0.551 ; Best_CR_auc: 0.404 ; Best_PR_auc: 0.450 ; loss: 6.104 ; negative_loss: -6.104\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "100%|██████████| 9/9 [00:00<00:00, 10.87it/s, loss=6.122]\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/avi/envs/pytorch3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "- Train metrics: survive_c_index: 0.629 ; nan_correlation: 00nan ; imputed.response_auc: 0.566 ; Response_SD/PD_auc: 0.632 ; Best_CR_auc: 0.500 ; Best_PR_auc: 0.500 ; loss: 6.134 ; negative_loss: -6.134\n",
      "- Eval metrics : survive_c_index: 0.528 ; nan_correlation: 0.000 ; imputed.response_auc: 0.585 ; Response_SD/PD_auc: 0.562 ; Best_CR_auc: 0.404 ; Best_PR_auc: 0.454 ; loss: 6.089 ; negative_loss: -6.089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# compute number of batches in one epoch (one full pass over the training set)\n",
    "for epoch in range(params.num_epochs):\n",
    "    train_metrics_all = []\n",
    "    val_metrics_all = []\n",
    "    if 'train' in dataloader.keys():\n",
    "        train_metrics = train_attention(models, optimizers, dataloader['train'], metrics, params, train_optimizer_mask)\n",
    "        train_metrics_all.append(train_metrics)\n",
    "        if params.tensorboardlog[0]:\n",
    "            writer.add_scalars('train_' + str(index), train_metrics, epoch)\n",
    "\n",
    "    # Evaluate for one epoch on validation set\n",
    "    if 'val' in dataloader.keys():\n",
    "        validation_file = os.path.join(tensorboard_dir, \"last_val_{0}.csv\".format(index)) if (epoch >= params.num_epochs - 1) else None\n",
    "        val_metrics = evaluate_attention(models, dataloader['val'], metrics, params, validation_file, writer=writer, epoch=epoch, index=index, tsne=tsne)\n",
    "        val_metrics_all.append(val_metrics)\n",
    "        if params.tensorboardlog[0]:\n",
    "            writer.add_scalars('val_' + str(index), val_metrics, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.tensorboardlog[1]:\n",
    "    if epoch % params.save_summary_steps == 0:\n",
    "\n",
    "        for model_name, model in zip(model_names, models):\n",
    "\n",
    "            for name, param1 in model.named_parameters():\n",
    "                try:\n",
    "                    writer.add_histogram(model_name + \"/\" + name, param1.clone().cpu().data.numpy(), epoch)\n",
    "                    writer.add_histogram(\"grad/\" + model_name + \"/\" + name, param1.grad.clone().cpu().data.numpy(), epoch)\n",
    "                except:\n",
    "                    # print(\"error in writing histogram\")\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_val_acc = None  # for cindex\n",
    "# tsne_params_log = 5\n",
    "\n",
    "for epoch in range(1):\n",
    "    # Run one epoch\n",
    "    logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "    train_metrics_all = []\n",
    "    val_metrics_all = []\n",
    "    for index, dataset in enumerate(datasets):\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        dataloader, train_optimizer_mask, (dataset_name, tsne) = dataset\n",
    "        if 'train' in dataloader.keys():\n",
    "            train_metrics = train_attention(models, optimizers, dataloader['train'], metrics, params, train_optimizer_mask)\n",
    "            train_metrics_all.append(train_metrics)\n",
    "            if params.tensorboardlog[0]:\n",
    "                writer.add_scalars('train_' + str(index), train_metrics, epoch)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        if 'val' in dataloader.keys():\n",
    "            validation_file = os.path.join(tensorboard_dir, \"last_val_{0}.csv\".format(index)) if (epoch >= params.num_epochs - 1) else None\n",
    "            val_metrics = evaluate_attention(models, dataloader['val'], metrics, params, validation_file, writer=writer, epoch=epoch, index=index, tsne=tsne)\n",
    "            val_metrics_all.append(val_metrics)\n",
    "            if params.tensorboardlog[0]:\n",
    "                writer.add_scalars('val_' + str(index), val_metrics, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
